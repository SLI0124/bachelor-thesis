cuda
Downloading: "https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth" to C:\Users\vojte/.cache\torch\hub\checkpoints\squeezenet1_1-b8a52dc0.pth
0.2%0.3%0.5%0.7%0.8%1.0%1.2%1.3%1.5%1.7%1.8%2.0%2.1%2.3%2.5%2.6%2.8%3.0%3.1%3.3%3.5%3.6%3.8%4.0%4.1%4.3%4.5%4.6%4.8%5.0%5.1%5.3%5.5%5.6%5.8%5.9%6.1%6.3%6.4%6.6%6.8%6.9%7.1%7.3%7.4%7.6%7.8%7.9%8.1%8.3%8.4%8.6%8.8%8.9%9.1%9.3%9.4%9.6%9.7%9.9%10.1%10.2%10.4%10.6%10.7%10.9%11.1%11.2%11.4%11.6%11.7%11.9%12.1%12.2%12.4%12.6%12.7%12.9%13.1%13.2%13.4%13.5%13.7%13.9%14.0%14.2%14.4%14.5%14.7%14.9%15.0%15.2%15.4%15.5%15.7%15.9%16.0%16.2%16.4%16.5%16.7%16.9%17.0%17.2%17.3%17.5%17.7%17.8%18.0%18.2%18.3%18.5%18.7%18.8%19.0%19.2%19.3%19.5%19.7%19.8%20.0%20.2%20.3%20.5%20.6%20.8%21.0%21.1%21.3%21.5%21.6%21.8%22.0%22.1%22.3%22.5%22.6%22.8%23.0%23.1%23.3%23.5%23.6%23.8%24.0%24.1%24.3%24.4%24.6%24.8%24.9%25.1%25.3%25.4%25.6%25.8%25.9%26.1%26.3%26.4%26.6%26.8%26.9%27.1%27.3%27.4%27.6%27.8%27.9%28.1%28.2%28.4%28.6%28.7%28.9%29.1%29.2%29.4%29.6%29.7%29.9%30.1%30.2%30.4%30.6%30.7%30.9%31.1%31.2%31.4%31.6%31.7%31.9%32.0%32.2%32.4%32.5%32.7%32.9%33.0%33.2%33.4%33.5%33.7%33.9%34.0%34.2%34.4%34.5%34.7%34.9%35.0%35.2%35.4%35.5%35.7%35.8%36.0%36.2%36.3%36.5%36.7%36.8%37.0%37.2%37.3%37.5%37.7%37.8%38.0%38.2%38.3%38.5%38.7%38.8%39.0%39.2%39.3%39.5%39.6%39.8%40.0%40.1%40.3%40.5%40.6%40.8%41.0%41.1%41.3%41.5%41.6%41.8%42.0%42.1%42.3%42.5%42.6%42.8%43.0%43.1%43.3%43.4%43.6%43.8%43.9%44.1%44.3%44.4%44.6%44.8%44.9%45.1%45.3%45.4%45.6%45.8%45.9%46.1%46.3%46.4%46.6%46.8%46.9%47.1%47.2%47.4%47.6%47.7%47.9%48.1%48.2%48.4%48.6%48.7%48.9%49.1%49.2%49.4%49.6%49.7%49.9%50.1%50.2%50.4%50.6%50.7%50.9%51.0%51.2%51.4%51.5%51.7%51.9%52.0%52.2%52.4%52.5%52.7%52.9%53.0%53.2%53.4%53.5%53.7%53.9%54.0%54.2%54.4%54.5%54.7%54.8%55.0%55.2%55.3%55.5%55.7%55.8%56.0%56.2%56.3%56.5%56.7%56.8%57.0%57.2%57.3%57.5%57.7%57.8%58.0%58.2%58.3%58.5%58.6%58.8%59.0%59.1%59.3%59.5%59.6%59.8%60.0%60.1%60.3%60.5%60.6%60.8%61.0%61.1%61.3%61.5%61.6%61.8%61.9%62.1%62.3%62.4%62.6%62.8%62.9%63.1%63.3%63.4%63.6%63.8%63.9%64.1%64.3%64.4%64.6%64.8%64.9%65.1%65.3%65.4%65.6%65.7%65.9%66.1%66.2%66.4%66.6%66.7%66.9%67.1%67.2%67.4%67.6%67.7%67.9%68.1%68.2%68.4%68.6%68.7%68.9%69.1%69.2%69.4%69.5%69.7%69.9%70.0%70.2%70.4%70.5%70.7%70.9%71.0%71.2%71.4%71.5%71.7%71.9%72.0%72.2%72.4%72.5%72.7%72.9%73.0%73.2%73.3%73.5%73.7%73.8%74.0%74.2%74.3%74.5%74.7%74.8%75.0%75.2%75.3%75.5%75.7%75.8%76.0%76.2%76.3%76.5%76.7%76.8%77.0%77.1%77.3%77.5%77.6%77.8%78.0%78.1%78.3%78.5%78.6%78.8%79.0%79.1%79.3%79.5%79.6%79.8%80.0%80.1%80.3%80.5%80.6%80.8%80.9%81.1%81.3%81.4%81.6%81.8%81.9%82.1%82.3%82.4%82.6%82.8%82.9%83.1%83.3%83.4%83.6%83.8%83.9%84.1%84.3%84.4%84.6%84.7%84.9%85.1%85.2%85.4%85.6%85.7%85.9%86.1%86.2%86.4%86.6%86.7%86.9%87.1%87.2%87.4%87.6%87.7%87.9%88.1%88.2%88.4%88.5%88.7%88.9%89.0%89.2%89.4%89.5%89.7%89.9%90.0%90.2%90.4%90.5%90.7%90.9%91.0%91.2%91.4%91.5%91.7%91.9%92.0%92.2%92.3%92.5%92.7%92.8%93.0%93.2%93.3%93.5%93.7%93.8%94.0%94.2%94.3%94.5%94.7%94.8%95.0%95.2%95.3%95.5%95.7%95.8%96.0%96.1%96.3%96.5%96.6%96.8%97.0%97.1%97.3%97.5%97.6%97.8%98.0%98.1%98.3%98.5%98.6%98.8%99.0%99.1%99.3%99.5%99.6%99.8%99.9%100.0%
Epoch 1/5
Batch [100/2073], Train Loss: 0.6055, Train Acc: 0.7898
Batch [200/2073], Train Loss: 0.3318, Train Acc: 0.8866
Batch [300/2073], Train Loss: 0.2419, Train Acc: 0.9177
Batch [400/2073], Train Loss: 0.1915, Train Acc: 0.9353
Batch [500/2073], Train Loss: 0.1604, Train Acc: 0.9461
Batch [600/2073], Train Loss: 0.1425, Train Acc: 0.9525
Batch [700/2073], Train Loss: 0.1273, Train Acc: 0.9575
Batch [800/2073], Train Loss: 0.1151, Train Acc: 0.9616
Batch [900/2073], Train Loss: 0.1065, Train Acc: 0.9647
Batch [1000/2073], Train Loss: 0.0987, Train Acc: 0.9675
Batch [1100/2073], Train Loss: 0.0914, Train Acc: 0.9699
Batch [1200/2073], Train Loss: 0.0856, Train Acc: 0.9719
Batch [1300/2073], Train Loss: 0.0806, Train Acc: 0.9736
Batch [1400/2073], Train Loss: 0.0769, Train Acc: 0.9748
Batch [1500/2073], Train Loss: 0.0734, Train Acc: 0.9760
Batch [1600/2073], Train Loss: 0.0700, Train Acc: 0.9771
Batch [1700/2073], Train Loss: 0.0669, Train Acc: 0.9782
Batch [1800/2073], Train Loss: 0.0645, Train Acc: 0.9791
Batch [1900/2073], Train Loss: 0.0622, Train Acc: 0.9799
Batch [2000/2073], Train Loss: 0.0600, Train Acc: 0.9807
Train Loss: 0.0586, Train Acc: 0.9811
Test Accuracy: 0.9972
Confusion Matrix:
[[13595    24]
 [   69 19469]]
Saved the new best model to ../data/models/80_20_split/pklot_ufpr05_squeezenet_default.pth
Epoch time: 5.863568079471588.4f minutes and 51.81408476829529.4f seconds
Epoch 2/5
Batch [100/2073], Train Loss: 0.0261, Train Acc: 0.9916
Batch [200/2073], Train Loss: 0.0369, Train Acc: 0.9893
Batch [300/2073], Train Loss: 0.0301, Train Acc: 0.9916
Batch [400/2073], Train Loss: 0.0279, Train Acc: 0.9921
Batch [500/2073], Train Loss: 0.0253, Train Acc: 0.9930
Batch [600/2073], Train Loss: 0.0229, Train Acc: 0.9937
Batch [700/2073], Train Loss: 0.0218, Train Acc: 0.9940
Batch [800/2073], Train Loss: 0.0210, Train Acc: 0.9943
Batch [900/2073], Train Loss: 0.0197, Train Acc: 0.9947
Batch [1000/2073], Train Loss: 0.0194, Train Acc: 0.9949
Batch [1100/2073], Train Loss: 0.0188, Train Acc: 0.9950
Batch [1200/2073], Train Loss: 0.0184, Train Acc: 0.9951
Batch [1300/2073], Train Loss: 0.0182, Train Acc: 0.9951
Batch [1400/2073], Train Loss: 0.0179, Train Acc: 0.9953
Batch [1500/2073], Train Loss: 0.0179, Train Acc: 0.9951
Batch [1600/2073], Train Loss: 0.0177, Train Acc: 0.9952
Batch [1700/2073], Train Loss: 0.0175, Train Acc: 0.9953
Batch [1800/2073], Train Loss: 0.0173, Train Acc: 0.9953
Batch [1900/2073], Train Loss: 0.0171, Train Acc: 0.9954
Batch [2000/2073], Train Loss: 0.0165, Train Acc: 0.9955
Train Loss: 0.0163, Train Acc: 0.9955
Test Accuracy: 0.9923
Confusion Matrix:
[[13383   236]
 [   18 19520]]
Saved the new best model to ../data/models/80_20_split/pklot_ufpr05_squeezenet_default.pth
Epoch time: 11.965921441713968.4f minutes and 57.955286502838135.4f seconds
Epoch 3/5
Batch [100/2073], Train Loss: 0.0145, Train Acc: 0.9950
Batch [200/2073], Train Loss: 0.0131, Train Acc: 0.9963
Batch [300/2073], Train Loss: 0.0121, Train Acc: 0.9966
Batch [400/2073], Train Loss: 0.0137, Train Acc: 0.9961
Batch [500/2073], Train Loss: 0.0156, Train Acc: 0.9957
Batch [600/2073], Train Loss: 0.0160, Train Acc: 0.9955
Batch [700/2073], Train Loss: 0.0173, Train Acc: 0.9951
Batch [800/2073], Train Loss: 0.0168, Train Acc: 0.9953
Batch [900/2073], Train Loss: 0.0177, Train Acc: 0.9950
Batch [1000/2073], Train Loss: 0.0185, Train Acc: 0.9948
Batch [1100/2073], Train Loss: 0.0184, Train Acc: 0.9949
Batch [1200/2073], Train Loss: 0.0177, Train Acc: 0.9951
Batch [1300/2073], Train Loss: 0.0178, Train Acc: 0.9951
Batch [1400/2073], Train Loss: 0.0170, Train Acc: 0.9953
Batch [1500/2073], Train Loss: 0.0165, Train Acc: 0.9955
Batch [1600/2073], Train Loss: 0.0161, Train Acc: 0.9956
Batch [1700/2073], Train Loss: 0.0156, Train Acc: 0.9957
Batch [1800/2073], Train Loss: 0.0153, Train Acc: 0.9958
Batch [1900/2073], Train Loss: 0.0151, Train Acc: 0.9959
Batch [2000/2073], Train Loss: 0.0148, Train Acc: 0.9960
Train Loss: 0.0148, Train Acc: 0.9960
Test Accuracy: 0.9981
Confusion Matrix:
[[13594    25]
 [   37 19501]]
Saved the new best model to ../data/models/80_20_split/pklot_ufpr05_squeezenet_default.pth
Epoch time: 18.396770222981772.4f minutes and 23.80621337890625.4f seconds
Epoch 4/5
Batch [100/2073], Train Loss: 0.0152, Train Acc: 0.9964
Batch [200/2073], Train Loss: 0.0140, Train Acc: 0.9964
Batch [300/2073], Train Loss: 0.0119, Train Acc: 0.9969
Batch [400/2073], Train Loss: 0.0112, Train Acc: 0.9970
Batch [500/2073], Train Loss: 0.0127, Train Acc: 0.9969
Batch [600/2073], Train Loss: 0.0123, Train Acc: 0.9970
Batch [700/2073], Train Loss: 0.0121, Train Acc: 0.9969
Batch [800/2073], Train Loss: 0.0120, Train Acc: 0.9970
Batch [900/2073], Train Loss: 0.0133, Train Acc: 0.9967
Batch [1000/2073], Train Loss: 0.0135, Train Acc: 0.9966
Batch [1100/2073], Train Loss: 0.0138, Train Acc: 0.9964
Batch [1200/2073], Train Loss: 0.0136, Train Acc: 0.9965
Batch [1300/2073], Train Loss: 0.0132, Train Acc: 0.9966
Batch [1400/2073], Train Loss: 0.0129, Train Acc: 0.9967
Batch [1500/2073], Train Loss: 0.0127, Train Acc: 0.9967
